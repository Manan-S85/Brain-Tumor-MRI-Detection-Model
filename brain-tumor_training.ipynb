{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2645886,"sourceType":"datasetVersion","datasetId":1608934},{"sourceId":10249205,"sourceType":"datasetVersion","datasetId":6339261}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import DenseNet121\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix\nimport seaborn as sns\nfrom sklearn.preprocessing import label_binarize\nimport ssl\n\nssl._create_default_https_context = ssl._create_unverified_context\n\nIMG_HEIGHT = 256  # Increased to avoid shrinking too much\nIMG_WIDTH = 256\nBATCH_SIZE = 32\n\ntrain_dir = '/kaggle/input/brain-tumor-mri-dataset/Training'\ntest_dir = '/kaggle/input/brain-tumor-mri-dataset/Testing'\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\nbase_model_densenet = DenseNet121(weights=None, include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\nbase_model_densenet.load_weights('/kaggle/input/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5')  # Manually downloaded weights\n\ncnn_model = models.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n    layers.MaxPooling2D(2, 2),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(4, activation='softmax')\n])\n\ncombined_input = layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n\ndensenet_output = base_model_densenet(combined_input)\ndensenet_output = layers.GlobalAveragePooling2D()(densenet_output)\ndensenet_output = layers.Dense(512, activation='relu')(densenet_output)\n\ncnn_output = cnn_model(combined_input)\n\nmerged_output = layers.concatenate([densenet_output, cnn_output])\n\nfinal_output = layers.Dense(4, activation='softmax')(merged_output)\n\nmodel = models.Model(inputs=combined_input, outputs=final_output)\n\nmodel.compile(\n    optimizer=Adam(learning_rate=0.0001),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()\n\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n    ModelCheckpoint('brain_tumor_cnn_best_model.keras', save_best_only=True),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n]\n\nsteps_per_epoch = 60\nvalidation_steps = 60\n\nhistory = model.fit(\n    train_generator,\n    epochs=25,  # Set the desired number of epochs\n    steps_per_epoch=steps_per_epoch,  # Set steps per epoch to 80\n    validation_data=test_generator,\n    validation_steps=validation_steps,  # Set validation steps to 80\n    verbose=1,\n    callbacks=callbacks\n)\n\nmodel.save('/kaggle/working/brain_tumor_cnn_model_enhanced.h5')\n\nmodel.save_weights('/kaggle/working/brain_tumor_cnn_weights_enhanced.weights.h5')\n\ntest_loss, test_acc = model.evaluate(test_generator)\nprint(f\"Test loss: {test_loss}, Test accuracy: {test_acc}\")\n\nmodel.save('brain_tumor_cnn_saved_model_enhanced')\n\ntrue_labels = test_generator.classes\npredictions = model.predict(test_generator, verbose=1)\n\ntrue_labels_one_hot = label_binarize(true_labels, classes=[0, 1, 2, 3])\n\nfpr, tpr, _ = roc_curve(true_labels_one_hot.ravel(), predictions.ravel())\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(8, 8))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc='lower right')\nplt.savefig('roc_curve.png')\nplt.show()\n\ncm = confusion_matrix(true_labels, np.argmax(predictions, axis=1))\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.savefig('confusion_matrix.png')\nplt.show()\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n# Save the plot as a file\nplt.savefig('accuracy_loss_plot_enhanced.png')\nplt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T20:10:05.306177Z","iopub.execute_input":"2024-12-19T20:10:05.306460Z","iopub.status.idle":"2024-12-20T05:31:38.464258Z","shell.execute_reply.started":"2024-12-19T20:10:05.306428Z","shell.execute_reply":"2024-12-20T05:31:38.462251Z"}},"outputs":[{"name":"stdout","text":"Found 5712 images belonging to 4 classes.\nFound 1311 images belonging to 4 classes.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ densenet121 (\u001b[38;5;33mFunctional\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │      \u001b[38;5;34m7,037,504\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ densenet121[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │        \u001b[38;5;34m524,800\u001b[0m │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ sequential (\u001b[38;5;33mSequential\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │     \u001b[38;5;34m31,510,084\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m516\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n│                           │                        │                │ sequential[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │          \u001b[38;5;34m2,068\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ densenet121 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,037,504</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ densenet121[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │     <span style=\"color: #00af00; text-decoration-color: #00af00\">31,510,084</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n│                           │                        │                │ sequential[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,068</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m39,074,456\u001b[0m (149.06 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">39,074,456</span> (149.06 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m38,990,808\u001b[0m (148.74 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">38,990,808</span> (148.74 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m83,648\u001b[0m (326.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">83,648</span> (326.75 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19s/step - accuracy: 0.7184 - loss: 0.7109 ","output_type":"stream"},{"name":"stderr","text":"/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1356s\u001b[0m 21s/step - accuracy: 0.7203 - loss: 0.7066 - val_accuracy: 0.5545 - val_loss: 1.1898 - learning_rate: 1.0000e-04\nEpoch 2/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1373s\u001b[0m 23s/step - accuracy: 0.8944 - loss: 0.2778 - val_accuracy: 0.6674 - val_loss: 1.1416 - learning_rate: 1.0000e-04\nEpoch 3/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1314s\u001b[0m 22s/step - accuracy: 0.9629 - loss: 0.1186 - val_accuracy: 0.8002 - val_loss: 0.6023 - learning_rate: 1.0000e-04\nEpoch 4/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1329s\u001b[0m 22s/step - accuracy: 0.9519 - loss: 0.1326 - val_accuracy: 0.9016 - val_loss: 0.2589 - learning_rate: 1.0000e-04\nEpoch 5/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1332s\u001b[0m 22s/step - accuracy: 0.9621 - loss: 0.1139 - val_accuracy: 0.7605 - val_loss: 0.7207 - learning_rate: 1.0000e-04\nEpoch 6/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1306s\u001b[0m 22s/step - accuracy: 0.9695 - loss: 0.0985 - val_accuracy: 0.9451 - val_loss: 0.1654 - learning_rate: 1.0000e-04\nEpoch 7/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1342s\u001b[0m 22s/step - accuracy: 0.9707 - loss: 0.0786 - val_accuracy: 0.9245 - val_loss: 0.1883 - learning_rate: 1.0000e-04\nEpoch 8/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1342s\u001b[0m 22s/step - accuracy: 0.9745 - loss: 0.0730 - val_accuracy: 0.9504 - val_loss: 0.1293 - learning_rate: 1.0000e-04\nEpoch 9/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1316s\u001b[0m 22s/step - accuracy: 0.9740 - loss: 0.0646 - val_accuracy: 0.9321 - val_loss: 0.2021 - learning_rate: 1.0000e-04\nEpoch 10/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1337s\u001b[0m 22s/step - accuracy: 0.9743 - loss: 0.0603 - val_accuracy: 0.9756 - val_loss: 0.0669 - learning_rate: 1.0000e-04\nEpoch 11/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1323s\u001b[0m 22s/step - accuracy: 0.9762 - loss: 0.0604 - val_accuracy: 0.9542 - val_loss: 0.1379 - learning_rate: 1.0000e-04\nEpoch 12/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1328s\u001b[0m 22s/step - accuracy: 0.9852 - loss: 0.0427 - val_accuracy: 0.9687 - val_loss: 0.1025 - learning_rate: 1.0000e-04\nEpoch 13/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1362s\u001b[0m 23s/step - accuracy: 0.9906 - loss: 0.0310 - val_accuracy: 0.9626 - val_loss: 0.1169 - learning_rate: 1.0000e-04\nEpoch 14/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1335s\u001b[0m 22s/step - accuracy: 0.9900 - loss: 0.0373 - val_accuracy: 0.9863 - val_loss: 0.0565 - learning_rate: 5.0000e-05\nEpoch 15/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1319s\u001b[0m 22s/step - accuracy: 0.9873 - loss: 0.0309 - val_accuracy: 0.9680 - val_loss: 0.0819 - learning_rate: 5.0000e-05\nEpoch 16/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1344s\u001b[0m 22s/step - accuracy: 0.9935 - loss: 0.0259 - val_accuracy: 0.9931 - val_loss: 0.0262 - learning_rate: 5.0000e-05\nEpoch 17/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1341s\u001b[0m 22s/step - accuracy: 0.9936 - loss: 0.0276 - val_accuracy: 0.9893 - val_loss: 0.0265 - learning_rate: 5.0000e-05\nEpoch 18/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1330s\u001b[0m 22s/step - accuracy: 0.9957 - loss: 0.0168 - val_accuracy: 0.9832 - val_loss: 0.0361 - learning_rate: 5.0000e-05\nEpoch 19/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1401s\u001b[0m 23s/step - accuracy: 0.9923 - loss: 0.0202 - val_accuracy: 0.9908 - val_loss: 0.0281 - learning_rate: 5.0000e-05\nEpoch 20/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1359s\u001b[0m 23s/step - accuracy: 0.9984 - loss: 0.0062 - val_accuracy: 0.9954 - val_loss: 0.0239 - learning_rate: 2.5000e-05\nEpoch 21/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1331s\u001b[0m 22s/step - accuracy: 0.9944 - loss: 0.0123 - val_accuracy: 0.9954 - val_loss: 0.0231 - learning_rate: 2.5000e-05\nEpoch 22/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1393s\u001b[0m 23s/step - accuracy: 0.9971 - loss: 0.0126 - val_accuracy: 0.9947 - val_loss: 0.0211 - learning_rate: 2.5000e-05\nEpoch 23/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1338s\u001b[0m 22s/step - accuracy: 0.9959 - loss: 0.0129 - val_accuracy: 0.9947 - val_loss: 0.0155 - learning_rate: 2.5000e-05\nEpoch 24/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1320s\u001b[0m 22s/step - accuracy: 0.9978 - loss: 0.0113 - val_accuracy: 0.9962 - val_loss: 0.0156 - learning_rate: 2.5000e-05\nEpoch 25/25\n\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1340s\u001b[0m 22s/step - accuracy: 0.9939 - loss: 0.0121 - val_accuracy: 0.9969 - val_loss: 0.0143 - learning_rate: 2.5000e-05\n\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 4s/step - accuracy: 0.9968 - loss: 0.0140\nTest loss: 0.014336504973471165, Test accuracy: 0.9969488978385925\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-5a7a4aae49e9>\u001b[0m in \u001b[0;36m<cell line: 132>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;31m# Optionally, save the model in a different format (e.g., TensorFlow SavedModel format)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'brain_tumor_cnn_saved_model_enhanced'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;31m# Get true labels and predictions for ROC Curve and Confusion Matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, zipped, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         )\n\u001b[0;32m--> 109\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"Invalid filepath extension for saving. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;34m\"Please add either a `.keras` extension for the native Keras \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=brain_tumor_cnn_saved_model_enhanced."],"ename":"ValueError","evalue":"Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=brain_tumor_cnn_saved_model_enhanced.","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\nimport seaborn as sns\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Paths to your dataset directories\ntrain_dir = '/kaggle/input/brain-tumor-mri-dataset/Training'\ntest_dir = '/kaggle/input/brain-tumor-mri-dataset/Testing'\n\n# Parameters\nimg_height, img_width = 256, 256\nbatch_size = 32\n\n# Data Preprocessing\ntrain_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\ntest_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n\n# Load test dataset\ntest_data = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=False\n)\n\n# Extract test images and labels\nX_test = np.concatenate([test_data[i][0] for i in range(len(test_data))], axis=0)\ny_test = np.concatenate([test_data[i][1] for i in range(len(test_data))], axis=0)\n\n# Load the trained model\nmodel = load_model('/kaggle/working/brain_tumor_cnn_model_enhanced.h5')\n\n# Make predictions\ny_pred_probs = model.predict(X_test)\n\n# For multi-class classification\nif y_test.shape[1] > 1:\n    y_pred = np.argmax(y_pred_probs, axis=1)\n    y_true = np.argmax(y_test, axis=1)\nelse:  # For binary classification\n    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n    y_true = y_test.flatten()\n\n# Calculate Accuracy\naccuracy = accuracy_score(y_true, y_pred)\n\n# Calculate F1 Score\nf1 = f1_score(y_true, y_pred, average='binary' if y_test.shape[1] == 1 else 'macro')\n\n# Classification Report\ntarget_names = list(test_data.class_indices.keys())\nreport = classification_report(y_true, y_pred, target_names=target_names)\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_true, y_pred)\n\n# Per-class Accuracy\nclassified_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n\n# Display metrics\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(\"\\nClassification Report:\\n\", report)\nprint(\"\\nClassified Accuracy (Per-class):\", classified_accuracy)\n\n# Create a single figure with subplots for multiple graphs\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\naxes = axes.flatten()\n\n# 1. Confusion Matrix\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names, cbar=False, ax=axes[0])\naxes[0].set_xlabel('Predicted Labels', fontsize=14)\naxes[0].set_ylabel('True Labels', fontsize=14)\naxes[0].set_title('Confusion Matrix', fontsize=16)\naxes[0].tick_params(axis='both', labelsize=12)\naxes[0].grid(True)\n\n# 2. ROC Curve (only for binary classification)\nif y_test.shape[1] == 1:\n    # For binary classification, extract probabilities for the positive class (class 1)\n    y_pred_probs_binary = y_pred_probs[:, 0] if y_pred_probs.ndim > 1 else y_pred_probs\n\n    fpr, tpr, thresholds = roc_curve(y_true, y_pred_probs_binary)\n    roc_auc = auc(fpr, tpr)\n\n    axes[1].plot(fpr, tpr, color='darkorange', lw=3, label=f'ROC curve (AUC = {roc_auc:.2f})')\n    axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    \n    axes[1].set_xlim([0.0, 1.0])\n    axes[1].set_ylim([0.0, 1.05])\n    axes[1].set_xticks(np.arange(0, 1.1, step=0.05))\n    axes[1].set_yticks(np.arange(0, 1.1, step=0.05))\n    axes[1].set_xlabel('False Positive Rate', fontsize=14)\n    axes[1].set_ylabel('True Positive Rate', fontsize=14)\n    axes[1].set_title('Receiver Operating Characteristic (ROC)', fontsize=16)\n    axes[1].legend(loc='lower right', fontsize=12)\n    axes[1].grid(True)\nelse:\n    # For multi-class classification (One-vs-Rest ROC curve)\n    n_classes = y_test.shape[1]\n    \n    fpr = {}\n    tpr = {}\n    roc_auc = {}\n    \n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_probs[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n        \n    for i in range(n_classes):\n        axes[1].plot(fpr[i], tpr[i], lw=3, label=f'Class {target_names[i]} (AUC = {roc_auc[i]:.2f})')\n\n    axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    \n    axes[1].set_xlim([0.0, 1.0])\n    axes[1].set_ylim([0.0, 1.05])\n    axes[1].set_xticks(np.arange(0, 1.1, step=0.05))\n    axes[1].set_yticks(np.arange(0, 1.1, step=0.05))\n    axes[1].set_xlabel('False Positive Rate', fontsize=14)\n    axes[1].set_ylabel('True Positive Rate', fontsize=14)\n    axes[1].set_title('Receiver Operating Characteristic (ROC) - Multi-Class', fontsize=16)\n    axes[1].legend(loc='lower right', fontsize=12)\n    axes[1].grid(True)\n\n# 3. Precision-Recall Curve\nif y_test.shape[1] > 1:  # Multi-class classification\n    for i, class_name in enumerate(target_names):\n        precision, recall, _ = precision_recall_curve(y_test[:, i], y_pred_probs[:, i])\n        axes[2].plot(recall, precision, lw=3, label=f'Class {class_name}')\n    axes[2].set_xlabel('Recall', fontsize=14)\n    axes[2].set_ylabel('Precision', fontsize=14)\n    axes[2].set_title('Precision-Recall Curve (Multi-Class)', fontsize=16)\nelse:  # Binary classification\n    precision, recall, _ = precision_recall_curve(y_true, y_pred_probs)\n    axes[2].plot(recall, precision, color='blue', lw=3)\n    axes[2].set_xlabel('Recall', fontsize=14)\n    axes[2].set_ylabel('Precision', fontsize=14)\n    axes[2].set_title('Precision-Recall Curve', fontsize=16)\n\naxes[2].set_xlim([0.0, 1.0])\naxes[2].set_ylim([0.0, 1.0])\naxes[2].set_xticks(np.arange(0, 1.1, step=0.05))\naxes[2].set_yticks(np.arange(0, 1.1, step=0.05))\naxes[2].grid(True)\naxes[2].legend(loc='lower left', fontsize=12)\n\n# Adjust layout\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:03:47.405666Z","iopub.execute_input":"2024-12-20T08:03:47.406141Z","execution_failed":"2024-12-20T08:09:57.357Z"}},"outputs":[{"name":"stdout","text":"Found 1311 images belonging to 4 classes.\n\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 4s/step\nAccuracy: 0.9969\nF1 Score: 0.9967\n\nClassification Report:\n               precision    recall  f1-score   support\n\n      glioma       1.00      0.99      0.99       300\n  meningioma       0.99      1.00      0.99       306\n     notumor       1.00      1.00      1.00       405\n   pituitary       1.00      1.00      1.00       300\n\n    accuracy                           1.00      1311\n   macro avg       1.00      1.00      1.00      1311\nweighted avg       1.00      1.00      1.00      1311\n\n\nClassified Accuracy (Per-class): [0.99333333 0.99673203 1.         0.99666667]\n","output_type":"stream"}],"execution_count":null}]}